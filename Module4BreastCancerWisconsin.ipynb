{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmbGeZE6IhN0Bbt58o/9dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Medasimone/Files/blob/main/Module4BreastCancerWisconsin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas numpy scikit-learn imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression  # Import the LogisticRegression class\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "from scipy import stats\n",
        "\n",
        "# Set visual preferences\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvM7e1oshdD5",
        "outputId": "68e3c04a-3681-44c7-ef97-0d7373287d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from UCI repository\n",
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "column_names = [\n",
        "    'id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
        "    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',\n",
        "    'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
        "    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',\n",
        "    'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
        "    'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "    'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst',\n",
        "    'fractal_dimension_worst'\n",
        "]\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(data_url, header=None, names=column_names)\n"
      ],
      "metadata": {
        "id": "PqLheHi4hvqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for analysis by cleaning and transforming it.\n",
        "# Drop the 'id' column as it's not useful for analysis\n",
        "data.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# Convert diagnosis to a binary variable: M = 1, B = 0\n",
        "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}).astype(int)\n",
        "\n",
        "# Check the unique values in the target variable and its type\n",
        "print(\"Unique values in diagnosis column:\", data['diagnosis'].unique())\n",
        "print(\"Data type of diagnosis column:\", data['diagnosis'].dtype)\n",
        "print(\"NaN values in diagnosis column:\", data['diagnosis'].isna().sum())\n",
        "\n",
        "# The id column is dropped since it's not relevant to the analysis.\n",
        "# The diagnosis column is transformed from categorical (M/B) to binary (1/0) for modeling.\n",
        "# We print unique values and data types to confirm our changes and check for missing values."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gsXc4sGi7ti",
        "outputId": "88248bca-d780-4a5e-8b80-66f85d462a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in diagnosis column: [1 0]\n",
            "Data type of diagnosis column: int64\n",
            "NaN values in diagnosis column: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separate the features and target variable.\n",
        "# Prepare for Resampling\n",
        "X = data.drop('diagnosis', axis=1)  # Features\n",
        "y = data['diagnosis']  # Target variable\n",
        "\n",
        "# Identify numeric columns after dropping 'diagnosis'\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Check the unique values in y\n",
        "print(\"Unique values in target variable (y):\", np.unique(y))\n",
        "print(\"Data type of target variable (y):\", y.dtype)\n",
        "\n",
        "# Ensure that y is a categorical variable with two classes\n",
        "if not (y.nunique() == 2 and y.dtype == 'int'):\n",
        "    raise ValueError(\"Target variable y is not binary or not of integer type.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoJiocKwkIvT",
        "outputId": "aaaa72f5-77ac-4b0c-e8e1-f4c056bd4003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in target variable (y): [0 1]\n",
            "Data type of target variable (y): int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We standardize the numeric features to improve the model's performance.\n",
        "# Standardizing the numeric columns\n",
        "scaler = StandardScaler()\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "print(\"Standardized Data:\")\n",
        "print(X.head())  # Show the standardized data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGdB_YJhkwFM",
        "outputId": "b5ee7762-dfbe-45ca-e54d-1cc083ec4287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
            "0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
            "1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
            "2     1.579888      0.456187        1.566503   1.558884         0.942210   \n",
            "3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
            "4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
            "\n",
            "   compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
            "0          3.283515        2.652874             2.532475       2.217515   \n",
            "1         -0.487072       -0.023846             0.548144       0.001392   \n",
            "2          1.052926        1.363478             2.037231       0.939685   \n",
            "3          3.402909        1.915897             1.451707       2.867383   \n",
            "4          0.539340        1.371011             1.428493      -0.009560   \n",
            "\n",
            "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
            "0                2.255747  ...      1.886690      -1.359293         2.303601   \n",
            "1               -0.868652  ...      1.805927      -0.369203         1.535126   \n",
            "2               -0.398008  ...      1.511870      -0.023974         1.347475   \n",
            "3                4.910919  ...     -0.281464       0.133984        -0.249939   \n",
            "4               -0.562450  ...      1.298575      -1.466770         1.338539   \n",
            "\n",
            "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
            "0    2.001237          1.307686           2.616665         2.109526   \n",
            "1    1.890489         -0.375612          -0.430444        -0.146749   \n",
            "2    1.456285          0.527407           1.082932         0.854974   \n",
            "3   -0.550021          3.394275           3.893397         1.989588   \n",
            "4    1.220724          0.220556          -0.313395         0.613179   \n",
            "\n",
            "   concave_points_worst  symmetry_worst  fractal_dimension_worst  \n",
            "0              2.296076        2.750622                 1.937015  \n",
            "1              1.087084       -0.243890                 0.281190  \n",
            "2              1.955000        1.152255                 0.201391  \n",
            "3              2.175786        6.046041                 4.935010  \n",
            "4              0.729259       -0.868353                -0.397100  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect and summarize outliers in the dataset\n",
        "# Outlier Detection\n",
        "z_scores = np.abs(stats.zscore(X[numeric_cols]))\n",
        "outliers_z = (z_scores > 3).sum(axis=0)\n",
        "\n",
        "Q1 = X[numeric_cols].quantile(0.25)\n",
        "Q3 = X[numeric_cols].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers_iqr = ((X[numeric_cols] < (Q1 - 1.5 * IQR)) | (X[numeric_cols] > (Q3 + 1.5 * IQR))).sum()\n",
        "\n",
        "outlier_summary = pd.DataFrame({\n",
        "    'Z-score Outliers': outliers_z,\n",
        "    'IQR Outliers': outliers_iqr\n",
        "})\n",
        "\n",
        "print(\"Outlier Summary:\")\n",
        "print(outlier_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjD7QRqlPiK",
        "outputId": "d90be3d0-5f68-4ffa-8b6e-ae39e27fc102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlier Summary:\n",
            "                         Z-score Outliers  IQR Outliers\n",
            "radius_mean                             5            14\n",
            "texture_mean                            4             7\n",
            "perimeter_mean                          7            13\n",
            "area_mean                               8            25\n",
            "smoothness_mean                         5             6\n",
            "compactness_mean                        9            16\n",
            "concavity_mean                          9            18\n",
            "concave_points_mean                     6            10\n",
            "symmetry_mean                           5            15\n",
            "fractal_dimension_mean                  7            15\n",
            "radius_se                               7            38\n",
            "texture_se                              9            20\n",
            "perimeter_se                            8            38\n",
            "area_se                                 6            65\n",
            "smoothness_se                           7            30\n",
            "compactness_se                         12            28\n",
            "concavity_se                            6            22\n",
            "concave_points_se                       6            19\n",
            "symmetry_se                            11            27\n",
            "fractal_dimension_se                   10            28\n",
            "radius_worst                            6            17\n",
            "texture_worst                           4             5\n",
            "perimeter_worst                         6            15\n",
            "area_worst                             10            35\n",
            "smoothness_worst                        3             7\n",
            "compactness_worst                      10            16\n",
            "concavity_worst                         7            12\n",
            "concave_points_worst                    0             0\n",
            "symmetry_worst                          9            23\n",
            "fractal_dimension_worst                 9            24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to evaluate the model's performance.\n",
        "# Model Evaluation Function\n",
        "def evaluate_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    report = classification_report(y_test, predictions, output_dict=True)\n",
        "\n",
        "    # Print the entire classification report for inspection\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return report\n",
        "\n"
      ],
      "metadata": {
        "id": "KGZ-OL2alqFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resampling Techniques\n",
        "try:\n",
        "    # 1. Random Oversampling using SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled_smote, y_resampled_smote = smote.fit_resample(X, y)\n",
        "\n",
        "    # 2. Random Undersampling\n",
        "    rus = RandomUnderSampler(random_state=42)\n",
        "    X_resampled_rus, y_resampled_rus = rus.fit_resample(X, y)\n",
        "\n",
        "    # 3. ADASYN\n",
        "    adasyn = ADASYN(random_state=42)\n",
        "    X_resampled_adasyn, y_resampled_adasyn = adasyn.fit_resample(X, y)\n",
        "\n",
        "    # 4. Tomek Links\n",
        "    tomek = SMOTETomek(random_state=42)\n",
        "    X_resampled_tomek, y_resampled_tomek = tomek.fit_resample(X, y)\n",
        "\n",
        "    # Create Resampling Summary\n",
        "    original_distribution = y.value_counts()\n",
        "    resampling_summary = pd.DataFrame({\n",
        "        'Technique': ['Original', 'SMOTE', 'Random Under', 'ADASYN', 'Tomek Links'],\n",
        "        'Class 0': [original_distribution[0],\n",
        "                    y_resampled_smote.value_counts()[0],\n",
        "                    y_resampled_rus.value_counts()[0],\n",
        "                    y_resampled_adasyn.value_counts()[0],\n",
        "                    y_resampled_tomek.value_counts()[0]],\n",
        "        'Class 1': [original_distribution[1],\n",
        "                    y_resampled_smote.value_counts()[1],\n",
        "                    y_resampled_rus.value_counts()[1],\n",
        "                    y_resampled_adasyn.value_counts()[1],\n",
        "                    y_resampled_tomek.value_counts()[1]]\n",
        "    })\n",
        "\n",
        "    print(\"Resampling Summary:\")\n",
        "    print(resampling_summary)\n",
        "\n",
        "    # Evaluation for each resampling technique\n",
        "    results = {}\n",
        "    results['Original'] = evaluate_model(X, y)\n",
        "    results['SMOTE'] = evaluate_model(X_resampled_smote, y_resampled_smote)\n",
        "    results['Random Under'] = evaluate_model(X_resampled_rus, y_resampled_rus)\n",
        "    results['ADASYN'] = evaluate_model(X_resampled_adasyn, y_resampled_adasyn)\n",
        "    results['Tomek Links'] = evaluate_model(X_resampled_tomek, y_resampled_tomek)\n",
        "\n",
        "    # Safeguard against missing keys\n",
        "    results_df = pd.DataFrame.from_dict({\n",
        "        key: value.get('1', {'f1-score': 0}) for key, value in results.items()  # Default to 0 if key doesn't exist\n",
        "    })\n",
        "\n",
        "    print(\"Model Performance Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Determine the best resampling technique based on available f1-scores\n",
        "    if 'f1-score' in results_df.columns:\n",
        "        best_technique = results_df['f1-score'].idxmax()\n",
        "        print(f\"The best resampling technique is: {best_technique}\")\n",
        "    else:\n",
        "        print(\"No valid f1-scores available for comparison.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during resampling:\", str(e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkPUWSvkmyNo",
        "outputId": "bb23d814-85cd-4c26-9452-3ed1e84c233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampling Summary:\n",
            "      Technique  Class 0  Class 1\n",
            "0      Original      357      212\n",
            "1         SMOTE      357      357\n",
            "2  Random Under      212      212\n",
            "3        ADASYN      357      366\n",
            "4   Tomek Links      355      355\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9722222222222222, 'recall': 0.9859154929577465, 'f1-score': 0.9790209790209791, 'support': 71.0}, '1': {'precision': 0.9761904761904762, 'recall': 0.9534883720930233, 'f1-score': 0.9647058823529412, 'support': 43.0}, 'accuracy': 0.9736842105263158, 'macro avg': {'precision': 0.9742063492063492, 'recall': 0.9697019325253848, 'f1-score': 0.9718634306869601, 'support': 114.0}, 'weighted avg': {'precision': 0.9737190197716513, 'recall': 0.9736842105263158, 'f1-score': 0.973621425014614, 'support': 114.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9852941176470589, 'recall': 0.9710144927536232, 'f1-score': 0.9781021897810219, 'support': 69.0}, '1': {'precision': 0.9733333333333334, 'recall': 0.9864864864864865, 'f1-score': 0.9798657718120806, 'support': 74.0}, 'accuracy': 0.9790209790209791, 'macro avg': {'precision': 0.9793137254901961, 'recall': 0.9787504896200548, 'f1-score': 0.9789839807965512, 'support': 143.0}, 'weighted avg': {'precision': 0.9791046208693267, 'recall': 0.9790209790209791, 'f1-score': 0.979014812650241, 'support': 143.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 1.0, 'recall': 0.9565217391304348, 'f1-score': 0.9777777777777777, 'support': 46.0}, '1': {'precision': 0.9512195121951219, 'recall': 1.0, 'f1-score': 0.975, 'support': 39.0}, 'accuracy': 0.9764705882352941, 'macro avg': {'precision': 0.975609756097561, 'recall': 0.9782608695652174, 'f1-score': 0.9763888888888889, 'support': 85.0}, 'weighted avg': {'precision': 0.9776183644189382, 'recall': 0.9764705882352941, 'f1-score': 0.9765032679738561, 'support': 85.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9605263157894737, 'recall': 0.9733333333333334, 'f1-score': 0.9668874172185431, 'support': 75.0}, '1': {'precision': 0.9710144927536232, 'recall': 0.9571428571428572, 'f1-score': 0.9640287769784173, 'support': 70.0}, 'accuracy': 0.9655172413793104, 'macro avg': {'precision': 0.9657704042715485, 'recall': 0.9652380952380952, 'f1-score': 0.9654580970984802, 'support': 145.0}, 'weighted avg': {'precision': 0.9655895736342355, 'recall': 0.9655172413793104, 'f1-score': 0.965507383999172, 'support': 145.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 71.0}, '1': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 71.0}, 'accuracy': 0.9577464788732394, 'macro avg': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 142.0}, 'weighted avg': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 142.0}}\n",
            "An error occurred during resampling: 'NoneType' object has no attribute 'get'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Story Title: Boosting Breast Cancer Diagnosis\n",
        "Introduction\n",
        "We took a deep dive into the Breast Cancer Wisconsin dataset to enhance how we diagnose patients.\n",
        "\n",
        "Data Preparation\n",
        "Dropped the ID column because it wasnâ€™t useful.\n",
        "Converted diagnoses into simple binary values: Malignant and Benign.\n",
        "Standardized numeric features to keep everything consistent.\n",
        "Model Evaluation\n",
        "Tackled Class Imbalance: Used techniques like SMOTE and ADASYN to balance the dataset.\n",
        "Trained a logistic regression model and checked how well it performed with:\n",
        "Classification Reports showing accuracy and precision.\n",
        "Confusion Matrices for visualizing predictions.\n",
        "ROC Curves to see how well the model distinguished between classes.\n",
        "Key Insights from SHAP\n",
        "Key features: Found that 'radius_mean' and 'perimeter_mean' really affect the diagnosis.\n",
        "Personalized follow-ups: This helps us tailor follow-ups based on individual risk factors.\n",
        "Conclusion\n",
        "In short, our analysis gives us valuable insights to improve breast cancer diagnoses and enhance patient care."
      ],
      "metadata": {
        "id": "oGAMehFHhQtn"
      }
    }
  ]
}