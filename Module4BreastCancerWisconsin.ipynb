{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmbGeZE6IhN0Bbt58o/9dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Medasimone/Files/blob/main/Module4BreastCancerWisconsin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas numpy scikit-learn imbalanced-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression  # Import the LogisticRegression class\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "from scipy import stats\n",
        "\n",
        "# Set visual preferences\n",
        "sns.set(style=\"whitegrid\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvM7e1oshdD5",
        "outputId": "68e3c04a-3681-44c7-ef97-0d7373287d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from UCI repository\n",
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
        "column_names = [\n",
        "    'id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n",
        "    'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave_points_mean',\n",
        "    'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se',\n",
        "    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se',\n",
        "    'concave_points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst',\n",
        "    'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
        "    'compactness_worst', 'concavity_worst', 'concave_points_worst', 'symmetry_worst',\n",
        "    'fractal_dimension_worst'\n",
        "]\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(data_url, header=None, names=column_names)\n"
      ],
      "metadata": {
        "id": "PqLheHi4hvqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for analysis by cleaning and transforming it.\n",
        "# Drop the 'id' column as it's not useful for analysis\n",
        "data.drop('id', axis=1, inplace=True)\n",
        "\n",
        "# Convert diagnosis to a binary variable: M = 1, B = 0\n",
        "data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0}).astype(int)\n",
        "\n",
        "# Check the unique values in the target variable and its type\n",
        "print(\"Unique values in diagnosis column:\", data['diagnosis'].unique())\n",
        "print(\"Data type of diagnosis column:\", data['diagnosis'].dtype)\n",
        "print(\"NaN values in diagnosis column:\", data['diagnosis'].isna().sum())\n",
        "\n",
        "# The id column is dropped since it's not relevant to the analysis.\n",
        "# The diagnosis column is transformed from categorical (M/B) to binary (1/0) for modeling.\n",
        "# We print unique values and data types to confirm our changes and check for missing values."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gsXc4sGi7ti",
        "outputId": "88248bca-d780-4a5e-8b80-66f85d462a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in diagnosis column: [1 0]\n",
            "Data type of diagnosis column: int64\n",
            "NaN values in diagnosis column: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separate the features and target variable.\n",
        "# Prepare for Resampling\n",
        "X = data.drop('diagnosis', axis=1)  # Features\n",
        "y = data['diagnosis']  # Target variable\n",
        "\n",
        "# Identify numeric columns after dropping 'diagnosis'\n",
        "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Check the unique values in y\n",
        "print(\"Unique values in target variable (y):\", np.unique(y))\n",
        "print(\"Data type of target variable (y):\", y.dtype)\n",
        "\n",
        "# Ensure that y is a categorical variable with two classes\n",
        "if not (y.nunique() == 2 and y.dtype == 'int'):\n",
        "    raise ValueError(\"Target variable y is not binary or not of integer type.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoJiocKwkIvT",
        "outputId": "aaaa72f5-77ac-4b0c-e8e1-f4c056bd4003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in target variable (y): [0 1]\n",
            "Data type of target variable (y): int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We standardize the numeric features to improve the model's performance.\n",
        "# Standardizing the numeric columns\n",
        "scaler = StandardScaler()\n",
        "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "\n",
        "print(\"Standardized Data:\")\n",
        "print(X.head())  # Show the standardized data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGdB_YJhkwFM",
        "outputId": "b5ee7762-dfbe-45ca-e54d-1cc083ec4287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
            "0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
            "1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
            "2     1.579888      0.456187        1.566503   1.558884         0.942210   \n",
            "3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
            "4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
            "\n",
            "   compactness_mean  concavity_mean  concave_points_mean  symmetry_mean  \\\n",
            "0          3.283515        2.652874             2.532475       2.217515   \n",
            "1         -0.487072       -0.023846             0.548144       0.001392   \n",
            "2          1.052926        1.363478             2.037231       0.939685   \n",
            "3          3.402909        1.915897             1.451707       2.867383   \n",
            "4          0.539340        1.371011             1.428493      -0.009560   \n",
            "\n",
            "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
            "0                2.255747  ...      1.886690      -1.359293         2.303601   \n",
            "1               -0.868652  ...      1.805927      -0.369203         1.535126   \n",
            "2               -0.398008  ...      1.511870      -0.023974         1.347475   \n",
            "3                4.910919  ...     -0.281464       0.133984        -0.249939   \n",
            "4               -0.562450  ...      1.298575      -1.466770         1.338539   \n",
            "\n",
            "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
            "0    2.001237          1.307686           2.616665         2.109526   \n",
            "1    1.890489         -0.375612          -0.430444        -0.146749   \n",
            "2    1.456285          0.527407           1.082932         0.854974   \n",
            "3   -0.550021          3.394275           3.893397         1.989588   \n",
            "4    1.220724          0.220556          -0.313395         0.613179   \n",
            "\n",
            "   concave_points_worst  symmetry_worst  fractal_dimension_worst  \n",
            "0              2.296076        2.750622                 1.937015  \n",
            "1              1.087084       -0.243890                 0.281190  \n",
            "2              1.955000        1.152255                 0.201391  \n",
            "3              2.175786        6.046041                 4.935010  \n",
            "4              0.729259       -0.868353                -0.397100  \n",
            "\n",
            "[5 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect and summarize outliers in the dataset\n",
        "# Outlier Detection\n",
        "z_scores = np.abs(stats.zscore(X[numeric_cols]))\n",
        "outliers_z = (z_scores > 3).sum(axis=0)\n",
        "\n",
        "Q1 = X[numeric_cols].quantile(0.25)\n",
        "Q3 = X[numeric_cols].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers_iqr = ((X[numeric_cols] < (Q1 - 1.5 * IQR)) | (X[numeric_cols] > (Q3 + 1.5 * IQR))).sum()\n",
        "\n",
        "outlier_summary = pd.DataFrame({\n",
        "    'Z-score Outliers': outliers_z,\n",
        "    'IQR Outliers': outliers_iqr\n",
        "})\n",
        "\n",
        "print(\"Outlier Summary:\")\n",
        "print(outlier_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzjD7QRqlPiK",
        "outputId": "d90be3d0-5f68-4ffa-8b6e-ae39e27fc102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlier Summary:\n",
            "                         Z-score Outliers  IQR Outliers\n",
            "radius_mean                             5            14\n",
            "texture_mean                            4             7\n",
            "perimeter_mean                          7            13\n",
            "area_mean                               8            25\n",
            "smoothness_mean                         5             6\n",
            "compactness_mean                        9            16\n",
            "concavity_mean                          9            18\n",
            "concave_points_mean                     6            10\n",
            "symmetry_mean                           5            15\n",
            "fractal_dimension_mean                  7            15\n",
            "radius_se                               7            38\n",
            "texture_se                              9            20\n",
            "perimeter_se                            8            38\n",
            "area_se                                 6            65\n",
            "smoothness_se                           7            30\n",
            "compactness_se                         12            28\n",
            "concavity_se                            6            22\n",
            "concave_points_se                       6            19\n",
            "symmetry_se                            11            27\n",
            "fractal_dimension_se                   10            28\n",
            "radius_worst                            6            17\n",
            "texture_worst                           4             5\n",
            "perimeter_worst                         6            15\n",
            "area_worst                             10            35\n",
            "smoothness_worst                        3             7\n",
            "compactness_worst                      10            16\n",
            "concavity_worst                         7            12\n",
            "concave_points_worst                    0             0\n",
            "symmetry_worst                          9            23\n",
            "fractal_dimension_worst                 9            24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to evaluate the model's performance.\n",
        "# Model Evaluation Function\n",
        "def evaluate_model(X, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    report = classification_report(y_test, predictions, output_dict=True)\n",
        "\n",
        "    # Print the entire classification report for inspection\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    return report\n",
        "\n"
      ],
      "metadata": {
        "id": "KGZ-OL2alqFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resampling Techniques\n",
        "try:\n",
        "    # 1. Random Oversampling using SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled_smote, y_resampled_smote = smote.fit_resample(X, y)\n",
        "\n",
        "    # 2. Random Undersampling\n",
        "    rus = RandomUnderSampler(random_state=42)\n",
        "    X_resampled_rus, y_resampled_rus = rus.fit_resample(X, y)\n",
        "\n",
        "    # 3. ADASYN\n",
        "    adasyn = ADASYN(random_state=42)\n",
        "    X_resampled_adasyn, y_resampled_adasyn = adasyn.fit_resample(X, y)\n",
        "\n",
        "    # 4. Tomek Links\n",
        "    tomek = SMOTETomek(random_state=42)\n",
        "    X_resampled_tomek, y_resampled_tomek = tomek.fit_resample(X, y)\n",
        "\n",
        "    # Create Resampling Summary\n",
        "    original_distribution = y.value_counts()\n",
        "    resampling_summary = pd.DataFrame({\n",
        "        'Technique': ['Original', 'SMOTE', 'Random Under', 'ADASYN', 'Tomek Links'],\n",
        "        'Class 0': [original_distribution[0],\n",
        "                    y_resampled_smote.value_counts()[0],\n",
        "                    y_resampled_rus.value_counts()[0],\n",
        "                    y_resampled_adasyn.value_counts()[0],\n",
        "                    y_resampled_tomek.value_counts()[0]],\n",
        "        'Class 1': [original_distribution[1],\n",
        "                    y_resampled_smote.value_counts()[1],\n",
        "                    y_resampled_rus.value_counts()[1],\n",
        "                    y_resampled_adasyn.value_counts()[1],\n",
        "                    y_resampled_tomek.value_counts()[1]]\n",
        "    })\n",
        "\n",
        "    print(\"Resampling Summary:\")\n",
        "    print(resampling_summary)\n",
        "\n",
        "    # Evaluation for each resampling technique\n",
        "    results = {}\n",
        "    results['Original'] = evaluate_model(X, y)\n",
        "    results['SMOTE'] = evaluate_model(X_resampled_smote, y_resampled_smote)\n",
        "    results['Random Under'] = evaluate_model(X_resampled_rus, y_resampled_rus)\n",
        "    results['ADASYN'] = evaluate_model(X_resampled_adasyn, y_resampled_adasyn)\n",
        "    results['Tomek Links'] = evaluate_model(X_resampled_tomek, y_resampled_tomek)\n",
        "\n",
        "    # Safeguard against missing keys\n",
        "    results_df = pd.DataFrame.from_dict({\n",
        "        key: value.get('1', {'f1-score': 0}) for key, value in results.items()  # Default to 0 if key doesn't exist\n",
        "    })\n",
        "\n",
        "    print(\"Model Performance Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    # Determine the best resampling technique based on available f1-scores\n",
        "    if 'f1-score' in results_df.columns:\n",
        "        best_technique = results_df['f1-score'].idxmax()\n",
        "        print(f\"The best resampling technique is: {best_technique}\")\n",
        "    else:\n",
        "        print(\"No valid f1-scores available for comparison.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"An error occurred during resampling:\", str(e))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkPUWSvkmyNo",
        "outputId": "bb23d814-85cd-4c26-9452-3ed1e84c233a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampling Summary:\n",
            "      Technique  Class 0  Class 1\n",
            "0      Original      357      212\n",
            "1         SMOTE      357      357\n",
            "2  Random Under      212      212\n",
            "3        ADASYN      357      366\n",
            "4   Tomek Links      355      355\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9722222222222222, 'recall': 0.9859154929577465, 'f1-score': 0.9790209790209791, 'support': 71.0}, '1': {'precision': 0.9761904761904762, 'recall': 0.9534883720930233, 'f1-score': 0.9647058823529412, 'support': 43.0}, 'accuracy': 0.9736842105263158, 'macro avg': {'precision': 0.9742063492063492, 'recall': 0.9697019325253848, 'f1-score': 0.9718634306869601, 'support': 114.0}, 'weighted avg': {'precision': 0.9737190197716513, 'recall': 0.9736842105263158, 'f1-score': 0.973621425014614, 'support': 114.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9852941176470589, 'recall': 0.9710144927536232, 'f1-score': 0.9781021897810219, 'support': 69.0}, '1': {'precision': 0.9733333333333334, 'recall': 0.9864864864864865, 'f1-score': 0.9798657718120806, 'support': 74.0}, 'accuracy': 0.9790209790209791, 'macro avg': {'precision': 0.9793137254901961, 'recall': 0.9787504896200548, 'f1-score': 0.9789839807965512, 'support': 143.0}, 'weighted avg': {'precision': 0.9791046208693267, 'recall': 0.9790209790209791, 'f1-score': 0.979014812650241, 'support': 143.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 1.0, 'recall': 0.9565217391304348, 'f1-score': 0.9777777777777777, 'support': 46.0}, '1': {'precision': 0.9512195121951219, 'recall': 1.0, 'f1-score': 0.975, 'support': 39.0}, 'accuracy': 0.9764705882352941, 'macro avg': {'precision': 0.975609756097561, 'recall': 0.9782608695652174, 'f1-score': 0.9763888888888889, 'support': 85.0}, 'weighted avg': {'precision': 0.9776183644189382, 'recall': 0.9764705882352941, 'f1-score': 0.9765032679738561, 'support': 85.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9605263157894737, 'recall': 0.9733333333333334, 'f1-score': 0.9668874172185431, 'support': 75.0}, '1': {'precision': 0.9710144927536232, 'recall': 0.9571428571428572, 'f1-score': 0.9640287769784173, 'support': 70.0}, 'accuracy': 0.9655172413793104, 'macro avg': {'precision': 0.9657704042715485, 'recall': 0.9652380952380952, 'f1-score': 0.9654580970984802, 'support': 145.0}, 'weighted avg': {'precision': 0.9655895736342355, 'recall': 0.9655172413793104, 'f1-score': 0.965507383999172, 'support': 145.0}}\n",
            "Classification Report:\n",
            "{'0': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 71.0}, '1': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 71.0}, 'accuracy': 0.9577464788732394, 'macro avg': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 142.0}, 'weighted avg': {'precision': 0.9577464788732394, 'recall': 0.9577464788732394, 'f1-score': 0.9577464788732394, 'support': 142.0}}\n",
            "An error occurred during resampling: 'NoneType' object has no attribute 'get'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Story Title: Boosting Breast Cancer Diagnosis\n",
        "Introduction\n",
        "We took a deep dive into the Breast Cancer Wisconsin dataset to enhance how we diagnose patients.\n",
        "\n",
        "Data Preparation\n",
        "Dropped the ID column because it wasn’t useful.\n",
        "Converted diagnoses into simple binary values: Malignant and Benign.\n",
        "Standardized numeric features to keep everything consistent.\n",
        "Model Evaluation\n",
        "Tackled Class Imbalance: Used techniques like SMOTE and ADASYN to balance the dataset.\n",
        "Trained a logistic regression model and checked how well it performed with:\n",
        "Classification Reports showing accuracy and precision.\n",
        "Confusion Matrices for visualizing predictions.\n",
        "ROC Curves to see how well the model distinguished between classes.\n",
        "Key Insights from SHAP\n",
        "Key features: Found that 'radius_mean' and 'perimeter_mean' really affect the diagnosis.\n",
        "Personalized follow-ups: This helps us tailor follow-ups based on individual risk factors.\n",
        "Conclusion\n",
        "In short, our analysis gives us valuable insights to improve breast cancer diagnoses and enhance patient care."
      ],
      "metadata": {
        "id": "oGAMehFHhQtn"
      }
    }
  ]
}